{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 13:28:40.591696: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO                                                    \n",
    "import gym   \n",
    "import gym_md                                                                           \n",
    "                                                                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/test1/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/alex/anaconda3/envs/test1/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/alex/anaconda3/envs/test1/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/alex/anaconda3/envs/test1/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/alex/anaconda3/envs/test1/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/alex/anaconda3/envs/test1/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.5     |\n",
      "|    ep_rew_mean     | -9.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.6        |\n",
      "|    ep_rew_mean          | -10.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 605         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010512962 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.92       |\n",
      "|    explained_variance   | 0.0199      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.24        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 22.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.4        |\n",
      "|    ep_rew_mean          | -8.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 589         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013570315 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.89       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.92        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 22.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.9        |\n",
      "|    ep_rew_mean          | -7.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 586         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012088863 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.87       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 27.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.96       |\n",
      "|    ep_rew_mean          | -4.96      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 585        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01566982 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.82      |\n",
      "|    explained_variance   | 0.215      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.1       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.98       |\n",
      "|    value_loss           | 23.7       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.3        |\n",
      "|    ep_rew_mean          | -4.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 580        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01634693 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.76      |\n",
      "|    explained_variance   | 0.1        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.89       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.974      |\n",
      "|    value_loss           | 23         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.14       |\n",
      "|    ep_rew_mean          | -2.14      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 577        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01760082 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.7       |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 10.9       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.965      |\n",
      "|    value_loss           | 20         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.7        |\n",
      "|    ep_rew_mean          | -1.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 575        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02137904 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -9.62      |\n",
      "|    explained_variance   | 0.14       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.72       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.952      |\n",
      "|    value_loss           | 12.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.46        |\n",
      "|    ep_rew_mean          | -0.46       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 569         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023247626 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.5        |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.41        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 9.72        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.28        |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 567         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029255893 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -9.37       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.23        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 7.35        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mmd-simple_0-v0\u001b[39m\u001b[39m\"\u001b[39m)                                                           \n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m PPO(policy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m,env \u001b[39m=\u001b[39m  env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)                               \n\u001b[0;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m25000\u001b[39;49m)                                                      \n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mppo_cartpole\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# saving the model to ppo_cartpole.zip                      \u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[39m=\u001b[39m PPO\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mppo_cartpole\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# loading the model from ppo_cartpole.zip             \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test1/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/test1/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    283\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/test1/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:272\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m# Optimization step\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 272\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    273\u001b[0m \u001b[39m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    274\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/anaconda3/envs/test1/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/test1/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"md-simple_0-v0\")                                                           \n",
    "                                                                                        \n",
    "model = PPO(policy = \"MlpPolicy\",env =  env, verbose=1,device=\"cuda\")                               \n",
    "model.learn(total_timesteps=25000)                                                      \n",
    "                                                                                        \n",
    "model.save(\"ppo_cartpole\")  # saving the model to ppo_cartpole.zip                      \n",
    "model = PPO.load(\"ppo_cartpole\")  # loading the model from ppo_cartpole.zip             \n",
    "                                                                                        \n",
    "obs = env.reset()                                                                       \n",
    "for i in range(1000):                                                                   \n",
    "    action, _state = model.predict(obs, deterministic=False)                             \n",
    "    obs, reward, done, info = env.step(action) \n",
    "    print(info)                                         \n",
    "    # env.render(mode='human')                                                                        \n",
    "    if done:                                                                            \n",
    "      obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shimmy\n",
      "  Downloading Shimmy-1.2.1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from shimmy) (1.24.3)\n",
      "Requirement already satisfied: gymnasium>=0.27.0 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from shimmy) (0.28.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from gymnasium>=0.27.0->shimmy) (4.6.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from gymnasium>=0.27.0->shimmy) (2.2.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from gymnasium>=0.27.0->shimmy) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from gymnasium>=0.27.0->shimmy) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/alex/anaconda3/envs/test1/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium>=0.27.0->shimmy) (3.15.0)\n",
      "Installing collected packages: shimmy\n",
      "Successfully installed shimmy-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install shimmy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/alex/Masters/gym-md/Logs/PP0/PPO_preTrained/md-hard-v0/PPO_md-hard-v0_0_0.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
